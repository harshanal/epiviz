#' Interpret Epidemiological Data or Visualisations using LLMs
#'
#' @description
#' `r lifecycle::badge("experimental")`
<<<<<<< HEAD
#' 
#' This function interprets a given data frame or ggplot visualisation by sending it to a language model API via the ellmer package. It supports multiple LLM providers, allowing users to specify the desired provider and model through environment variables.
=======
#'
#' This function interprets a given data frame or ggplot visualisation by sending it to a language model API via the elmer package. It supports multiple LLM providers, allowing users to specify the desired provider and model through environment variables.
>>>>>>> 2a34b14 (update LLM models and changes based on LLM)
#'
#' @param input An input object, either a data frame or a ggplot object, representing the data or visualization to be interpreted.
#' @param word_limit Integer. The desired word length for the response. Defaults to 100.
#' @param prompt_extension Character. Optional additional instructions to extend the standard prompt. Defaults to NULL.
#' @return A character string containing the narrative or interpretation of the input object as generated by the LLM.
#' @details
#' **Supported LLM Providers and Models:**
#' - **OpenAI**: Utilizes OpenAI's models via `chat_openai()`. Requires setting the `OPENAI_API_KEY` environment variable. Applicable models include:
#'   - `"gpt-4o"`
#'   - `"gpt-4o-mini"`
#'   - `"o1-mini"`
#' - **Google Gemini**: Utilizes Google's Gemini models via `chat_gemini()`. Requires setting the `GOOGLE_API_KEY` environment variable. Applicable models include:
#'   - `"gemini-1.5-flash"`
#' - **Anthropic Claude**: Utilizes Anthropic's Claude models via `chat_claude()`. Requires setting the `CLAUDE_API_KEY` environment variable. Applicable models include:
#'   - `"claude-1"`
#'
#' **Environment Variables:**
#' - `LLM_PROVIDER`: Specifies the LLM provider ("openai", "gemini", "claude").
#' - `LLM_API_KEY`: The API key corresponding to the chosen provider.
#' - `LLM_MODEL`: The model identifier to use.
#'
#' **Note:** Ensure that the appropriate environment variables are set before invoking this function. The function will throw an error if the specified provider is unsupported or if required environment variables are missing.
#'
#' @import ggplot2
#' @importFrom jsonlite toJSON
#' @import ellmer
#' @importFrom lifecycle badge signal_stage
#' @export
#'
#' @section Tested Models:
#' As of October 2025, this function has been tested and verified to work with the following models:
#' - OpenAI: gpt-4.1-nano
#' - Anthropic: claude-sonnet-4-20250514  
#' - Google Gemini: gemini-2.5-flash-lite
#' 
#' Additional models may be tested in the future. Users can provide custom instructions
#' through the \code{prompt_extension} parameter for specialised analysis requirements.
llm_interpret <- function(input,
                          word_limit = 100,
                          prompt_extension = NULL) {
  lifecycle::signal_stage("experimental", "llm_interpret()")

  # Validate input parameters
  if (!is.numeric(word_limit) || word_limit <= 0) {
    stop("word_limit must be a positive number")
  }

  if (!is.null(prompt_extension) && !is.character(prompt_extension)) {
    stop("prompt_extension must be NULL or a character string")
  }

  # Check for required environment variables with informative messages
  tryCatch({
    provider <- Sys.getenv("LLM_PROVIDER")
    if (provider == "") {
      stop("LLM_PROVIDER environment variable is not set. ",
           "Please set it to one of: 'openai', 'gemini', or 'claude'")
    }

    api_key <- Sys.getenv("LLM_API_KEY")
    if (api_key == "") {
      stop("LLM_API_KEY environment variable is not set. ",
           sprintf("For %s, please set the appropriate API key.",
                   toupper(provider)))
    }

    model <- Sys.getenv("LLM_MODEL")
    if (model == "") {
      stop("LLM_MODEL environment variable is not set. ",
           sprintf("For %s, please set an appropriate model identifier.",
                   toupper(provider)))
    }
  }, error = function(e) {
    stop("Error checking environment variables: ", conditionMessage(e))
  })

  # Initialize the chat client with error handling
  chat <- tryCatch({
    switch(
      provider,
      "openai" = chat_openai(model = model, api_key = api_key),
      "gemini" = chat_google_gemini(model = model, api_key = api_key),
      "anthropic" = chat_anthropic(model = model, api_key = api_key),
      stop(sprintf("Unsupported LLM provider: '%s'", provider))
    )
  }, error = function(e) {
    stop("Failed to initialize chat client: ", conditionMessage(e))
  })

  # Set the system prompt with error handling
  tryCatch({
    chat$set_system_prompt(
      "You are a concise assistant focusing on epidemiologically relevant observations."
    )
  }, error = function(e) {
    stop("Failed to set system prompt: ", conditionMessage(e))
  })

  # Define the standard prompt
  standard_prompt <- tryCatch({
    prompt <- paste(
      "Please provide an interpretation focusing on the most",
      "epidemiologically relevant observations in",
      word_limit,
      "words or fewer."
    )
    if (!is.null(prompt_extension)) {
      prompt <- paste(prompt, prompt_extension)
    }
    prompt
  }, error = function(e) {
    stop("Failed to construct prompt: ", conditionMessage(e))
  })

  # Process input based on type
  if (is.data.frame(input)) {
    if (nrow(input) == 0) {
      stop("Input data frame is empty")
    }

    # Convert data frame to JSON with error handling
    json_data <- tryCatch({
      jsonlite::toJSON(input, pretty = TRUE, auto_unbox = TRUE)
    }, error = function(e) {
      stop("Failed to convert data frame to JSON: ", conditionMessage(e))
    })

    # Send JSON data to the API with error handling
    tryCatch({
      response <- chat$chat(standard_prompt, json_data)
      return(response)
    }, error = function(e) {
      stop("API request failed for data frame input: ", conditionMessage(e))
    })
  }
  else if (inherits(input, "ggplot")) {
    temp_file <- NULL

    # Use withCallingHandlers for cleanup even if error occurs
    withCallingHandlers({
      # Save the ggplot object as a temporary image
      temp_file <- tempfile(fileext = ".png")
      tryCatch({
        ggplot2::ggsave(
          temp_file,
          plot = input,
          width = 6,
          height = 4,
          dpi = 300
        )
      }, error = function(e) {
        stop("Failed to save plot as image: ", conditionMessage(e))
      })

      # Send image file to the API
      response <- tryCatch({
        chat$chat(content_image_file(temp_file), standard_prompt)
      }, error = function(e) {
        stop("API request failed for plot input: ", conditionMessage(e))
      })

      return(as.character(response))
    }, finally = {
      # Clean up temporary file
      if (!is.null(temp_file) && file.exists(temp_file)) {
        unlink(temp_file)
      }
    })
  }
  else {
    stop(
      sprintf(
        "Unsupported input type: %s. Input must be a data frame or ggplot object.",
        class(input)[1]
      )
    )
  }
}
